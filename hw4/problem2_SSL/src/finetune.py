# -*- coding: utf-8 -*-
"""1-1train.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1OTO73ALmeL1DrDPhWUVt5Ycod00vll3w
"""

import csv
import torch
import json
import os
import torch.nn as nn
import torchvision.transforms as T
import numpy as np
from torch.utils.data import DataLoader
from tqdm.auto import tqdm
import random
import torchvision.models as models
from dataset import FineTuneDataset
from datetime import datetime
import argparse
import json
from model import ImgClassifier


def config_parser():
    parser = argparse.ArgumentParser(
        formatter_class=argparse.ArgumentDefaultsHelpFormatter
    )
    parser.add_argument("--ckpt", default="", help="ckpt to load")
    parser.add_argument("--name", default="test", help="pretrained model name")
    parser.add_argument("--cuda", default="cuda", help="choose a cuda")
    parser.add_argument(
        "--config",
        default="/home/stan/hw4-stanthemaker/problem2_SSL/src/config_fine_tune.json",
        help="path to config.json",
    )
    parser.add_argument("--A", action="store_true")
    parser.add_argument("--B", action="store_true")
    parser.add_argument("--C", action="store_true")
    parser.add_argument("--D", action="store_true")
    parser.add_argument("--E", action="store_true")
    return parser


myseed = 1314520  # set a random seed for reproducibility
torch.backends.cudnn.deterministic = True
torch.backends.cudnn.benchmark = False
np.random.seed(myseed)
torch.manual_seed(myseed)
if torch.cuda.is_available():
    torch.cuda.manual_seed_all(myseed)

parser = config_parser()
args = parser.parse_args()
device = args.cuda if torch.cuda.is_available() else "cpu"
print(f"using device {device}")

with open(args.config, "r") as f:
    config = json.load(f)

time = datetime.now().strftime("%m%d-%H%M_")
train_name = time + args.name
log_path = os.path.join(
    "/home/stan/hw4-stanthemaker/problem2_SSL/log", f"finetune_{train_name}.txt"
)
assert args.A or args.B or args.C or args.D or args.E == True

train_tfm = T.Compose(
    [
        T.Resize(224),
        T.CenterCrop(224),
        # T.RandomRotation(20),
        T.ToTensor(),
        T.Normalize(
            mean=torch.tensor([0.485, 0.456, 0.406]),
            std=torch.tensor([0.229, 0.224, 0.225]),
        ),
    ]
)

valid_tfm = T.Compose(
    [
        T.Resize(224),
        T.CenterCrop(224),
        T.ToTensor(),
        T.Normalize(
            mean=torch.tensor([0.485, 0.456, 0.406]),
            std=torch.tensor([0.229, 0.224, 0.225]),
        ),
    ]
)

label_dict = json.load(
    open("/home/stan/hw4-stanthemaker/hw4_data/office/label_ids.json")
)

train_set = FineTuneDataset(
    os.path.join(config["dataset_dir"], "train"), label_dict, train_tfm
)
train_loader = DataLoader(
    train_set,
    batch_size=config["batch_size"],
    shuffle=True,
    num_workers=0,
    pin_memory=True,
)
val_set = FineTuneDataset(
    os.path.join(config["dataset_dir"], "val"), label_dict, valid_tfm
)
val_loader = DataLoader(
    val_set,
    batch_size=config["batch_size"],
    shuffle=True,
    num_workers=0,
    pin_memory=True,
)
model = None
"""
Choose different settings to complete report 
"""
if args.A:
    print("training from scratch")
elif args.B:
    print("load TA pretrained SSL weights")
    model = models.resnet50(weights=None)
    model.load_state_dict(torch.load(config["TAbackbone"]))
elif args.C:
    print("load my pretrained SSL weights")
    model = ImgClassifier(device)
    model.resnet.load_state_dict(torch.load(config["mybackbone"])["model"])
elif args.D:
    print("load TA pretrained SSL weights and freeze")
    model = models.resnet50(weights=None)
    model.load_state_dict(torch.load(config["TAbackbone"]))
    for name, param in model.named_parameters():
        if not name.startswith("fc"):
            param.requires_grad = False
elif args.E:
    print("load my pretrained SSL weights and freeze")
    model.load_state_dict(torch.load(config["mybackbone"]))
    for name, param in model.parnamed_parametersameters():
        if not name.startswith("fc"):
            param.requires_grad = False

print(model)
model = model.to(device)


num_epoch = config["num_epoch"]
patience = config["patience"]
criterion = nn.CrossEntropyLoss(label_smoothing=0.1)
optimizer = torch.optim.SGD(
    model.parameters(), lr=5e-4, momentum=0.9, weight_decay=2e-5
)
# optimizer.load_state_dict(state['optimizer'])
scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(
    optimizer, mode="max", factor=0.8, patience=5, min_lr=1e-5, verbose=True
)
# scheduler.load_state_dict(state['scheduler'])

stale = 0
best_acc = 0

for epoch in range(num_epoch):

    # train
    model.train()
    train_loss = []
    train_accs = []

    for batch in tqdm(train_loader):

        imgs, labels = batch

        logits = model(imgs.to(device))

        loss = criterion(logits, labels.to(device))
        loss.backward()
        grad_norm = nn.utils.clip_grad_norm_(model.parameters(), max_norm=10)

        optimizer.step()

        acc = (logits.argmax(dim=-1) == labels.to(device)).float().mean()

        train_loss.append(loss.item())
        train_accs.append(acc)

    train_loss = sum(train_loss) / len(train_loss)
    train_acc = sum(train_accs) / len(train_accs)

    with open(log_path, "a") as f:
        f.write(
            f"[ Train | {epoch + 1:03d}/{num_epoch:03d} ] loss = {train_loss:.4f}, best_acc = {best_acc:.3f}\n"
        )

    print(
        f"[ Train | {epoch + 1:03d}/{num_epoch:03d} ] loss = {train_loss:.5f}, acc = {train_acc:.5f}"
    )

    # validation
    model.eval()

    val_loss = []
    val_accs = []
    for batch in tqdm(val_loader):

        imgs, labels = batch

        with torch.no_grad():
            logits = model(imgs.to(device))

        loss = criterion(logits, labels.to(device))

        acc = (logits.argmax(dim=-1) == labels.to(device)).float().mean()

        val_loss.append(loss.item())
        val_accs.append(acc)

    val_loss = sum(val_loss) / len(val_loss)
    val_acc = sum(val_accs) / len(val_accs)

    print(
        f"[ Valid | {epoch + 1:03d}/{num_epoch:03d} ] loss = {val_loss:.5f}, acc = {val_acc:.5f}"
    )
    with open(log_path, "a") as f:
        f.write(
            f"[ Valid | {epoch + 1:03d}/{num_epoch:03d} ] loss = {val_loss:.4f}, acc = {val_acc:.5f}, best_acc = {best_acc:.3f}\n"
        )

    # update log
    if val_acc > best_acc:
        with open(log_path, "a") as f:
            f.write(f"best model found at epoch {epoch}, saving\n")
        state = {
            "epoch": epoch,
            "model": model.state_dict(),
            "optimizer": optimizer.state_dict(),
            "scheduler": scheduler.state_dict(),
        }

        torch.save(state, os.path.join(config["ckpt_dir"], f"{train_name}_finetune.pt"))
        best_acc = val_acc
        stale = 0
    else:
        stale += 1
        if stale > patience:
            print(f"no improvement for {patience} consecutive epochs, early stopping")
            break
    scheduler.step(val_acc)
